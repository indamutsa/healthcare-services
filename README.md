# Healthcare Clinical Trials Platform

A comprehensive clinical trials data processing system built with **IBM MQ**, **Spring Boot**, and **microservices architecture** for healthcare environments.

## Architecture Overview

This platform processes clinical trial data through a **message-driven architecture**:

```
Clinical Trial Sites â†’ REST API â†’ Spring JMS â†’ IBM MQ â†’ Lab Processor â†’ EMR/Audit
```

**Core Components:**
- **Clinical Data Gateway** (Port 8080): Receives and validates clinical trial data via REST API
- **Lab Results Processor** (Port 8081): Processes lab results, updates EMR systems, and handles audit logging
- **IBM MQ**: Message broker for reliable clinical data transmission
- **Monitoring Stack**: Prometheus, Grafana, ELK for system observability

## Quick Start (Container-Based Development)

### 1. Setup Development Environment
```bash
# Open VS Code with Alpine container (clean machine approach)
# Run terminal enhancement
chmod +x enhance-terminal.sh
./enhance-terminal.sh

# Setup complete development environment
chmod +x alpine-setup.sh
./alpine-setup.sh
```

### 2. Start the Platform
```bash
# Activate development environment
source .pyenv/bin/activate

# Start IBM MQ container
docker compose up -d

# Build and run Clinical Data Gateway
cd applications/clinical-data-gateway
mvn clean package
java -jar target/clinical-data-gateway-1.0.0.jar

# Generate test clinical data (new terminal)
python operations/scripts/demo/clinical_data_generator.py --count 10 --interval 2-5
```

### 3. Verify the Demo
```bash
# Test gateway health
curl http://localhost:8080/api/clinical/health

# View processing statistics
curl http://localhost:8080/api/clinical/stats
```

## Project Structure

```
clinical-trials-service/
â”œâ”€â”€ .devcontainer/              # VS Code container configuration
â”œâ”€â”€ applications/               # Spring Boot microservices
â”‚   â”œâ”€â”€ clinical-data-gateway/  # REST API & JMS producer
â”‚   â””â”€â”€ lab-results-processor/  # JMS consumer & data processor
â”œâ”€â”€ operations/                 # Demo scripts and automation
â”‚   â””â”€â”€ scripts/demo/          # Clinical data generator
â”œâ”€â”€ infrastructure/             # IBM MQ Docker configuration  
â”œâ”€â”€ monitoring/                 # Prometheus, Grafana, ELK stack
â”œâ”€â”€ config/                     # Environment configurations
â”œâ”€â”€ docs/                       # API and architecture documentation
â”œâ”€â”€ test-data/                  # Sample clinical data for testing
â””â”€â”€ scripts/                    # Build and deployment utilities
```

## Clinical Data Types

The system processes four types of clinical data:

1. **Patient Demographics** (15%): Age, gender, weight, height, study enrollment
2. **Vital Signs** (40%): Blood pressure, heart rate, temperature, oxygen saturation
3. **Lab Results** (30%): Blood glucose, cholesterol, hemoglobin, liver enzymes
4. **Adverse Events** (15%): Side effects, severity, relation to study drug

## Development Workflow

### Container-First Approach
This project uses **containerized development** to avoid installing tools locally:

```bash
# 1. VS Code + Alpine container (clean slate)
# 2. Enhanced terminal (ZSH + autosuggestions)  
# 3. Complete toolchain (Java 17, Maven, Python, Docker)
# 4. Ready for demo in minutes
```

### Build & Test
```bash
# Build all services
./scripts/build-all.sh

# Run integration tests
./scripts/test-complete-flow.sh

# Clean build artifacts
./scripts/clean-all.sh
```

## Technology Stack

**Backend:**
- **Java 17** with Spring Boot 3.2
- **Spring JMS** for message processing
- **IBM MQ** for reliable messaging
- **Maven** for build management

**Data Generation:**
- **Python 3** with realistic clinical data
- **Faker** for generating HIPAA-compliant test data
- **REST client** for API integration

**Infrastructure:**
- **Docker Compose** for local development
- **Alpine Linux** for lightweight containers
- **ZSH** with enhanced terminal experience

## HIPAA Compliance Features

- **Anonymized Patient IDs**: Format `PT12AB34CD` (no real identifiers)
- **Medical Validation**: Realistic ranges for vitals, labs, and adverse events
- **Audit Trails**: Complete message tracking through the pipeline
- **Secure Transmission**: IBM MQ with persistent message delivery
- **Data Retention**: 7-year retention policy for regulatory compliance

## Monitoring & Observability

- **Health Checks**: `/api/clinical/health` endpoint
- **Metrics**: Processing statistics and success rates
- **Logging**: Structured logs for debugging and compliance
- **Message Tracking**: Unique message IDs through entire pipeline

```bash
# Access monitoring dashboards
Grafana:    http://localhost:3000
Prometheus: http://localhost:9090
Kibana:     http://localhost:5601
```

## Interview Demo Flow

This platform demonstrates several key concepts:

1. **Microservices Architecture**: Loosely coupled services with clear boundaries
2. **Message-Driven Design**: Asynchronous processing with IBM MQ
3. **Domain Modeling**: Healthcare entities with proper validation
4. **Container Development**: Complete environment without local installations
5. **Production Readiness**: Monitoring, logging, and error handling

### Demo Script
```bash
# Terminal 1: Start gateway
java -jar applications/clinical-data-gateway/target/clinical-data-gateway-1.0.0.jar

# Terminal 2: Generate data  
python operations/scripts/demo/clinical_data_generator.py --verbose

# Terminal 3: Monitor
curl -s http://localhost:8080/api/clinical/stats | jq
```

## API Documentation

### POST /api/clinical/data
Receives clinical trial data from healthcare sites.

**Example Payload:**
```json
{
  "message_id": "550e8400-e29b-41d4-a716-446655440000",
  "timestamp": "2025-09-08T14:30:45.123456",
  "study_phase": "Phase II",
  "site_id": "SITE001",
  "vital_signs": {
    "patient_id": "PT12AB34CD",
    "systolic_bp": 128,
    "diastolic_bp": 82,
    "heart_rate": 74,
    "temperature_celsius": 37.1
  }
}
```

## Contributing

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Commit** changes: `git commit -m 'Add amazing feature'`
4. **Push** to branch: `git push origin feature/amazing-feature`
5. **Open** a Pull Request

## License

MIT License - see [LICENSE](LICENSE) file for details.

ğŸ¥

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLINICAL MLOPS DATA PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. STREAMING INGESTION (Real-time events)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kafka Producer â†’ Kafka Topics â†’ Kafka Consumer             â”‚
â”‚  â†“                                                           â”‚
â”‚  MinIO Bronze Layer (raw/topic/date=YYYY-MM-DD/hour=HH/)   â”‚
â”‚  Format: JSON (newline-delimited)                           â”‚
â”‚  Purpose: Immutable raw event capture                       â”‚
â”‚  Retention: 90 days                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. DATA PROCESSING (Clean & validate)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Spark Jobs (Bronze â†’ Silver transformation)                â”‚
â”‚  â†“                                                           â”‚
â”‚  MinIO Silver Layer (processed/topic/date=YYYY-MM-DD/)     â”‚
â”‚  Format: Parquet (compressed, columnar)                     â”‚
â”‚  Purpose: ML-ready, deduplicated, validated data            â”‚
â”‚  Retention: 2 years                                         â”‚
â”‚                                                              â”‚
â”‚  Transformations:                                            â”‚
â”‚  â€¢ Deduplication (patient_id, timestamp, source)            â”‚
â”‚  â€¢ Validation (physiological ranges)                        â”‚
â”‚  â€¢ Standardization (units, formats)                         â”‚
â”‚  â€¢ Enrichment (derived metrics)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. FEATURE ENGINEERING (Create ML features)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Feature Engineering Pipeline (Spark/Python)                â”‚
â”‚  â†“                                                           â”‚
â”‚  Creates 120+ features:                                     â”‚
â”‚  â€¢ Temporal: rolling windows (1h, 6h, 24h)                 â”‚
â”‚  â€¢ Derived: pulse pressure, MAP, trends                     â”‚
â”‚  â€¢ Aggregations: patient-level statistics                   â”‚
â”‚  â€¢ Context: demographics, trial arm                         â”‚
â”‚                                                              â”‚
â”‚  Writes to DUAL STORAGE:                                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  OFFLINE FEATURE STORE                       â”‚          â”‚
â”‚  â”‚  (Historical training & batch scoring)       â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚  Storage: MinIO (S3)                         â”‚          â”‚
â”‚  â”‚  Location: features/offline/date=YYYY-MM-DD/ â”‚          â”‚
â”‚  â”‚  Format: Parquet (partitioned by date)       â”‚          â”‚
â”‚  â”‚  Schema:                                      â”‚          â”‚
â”‚  â”‚    - patient_id                               â”‚          â”‚
â”‚  â”‚    - timestamp                                â”‚          â”‚
â”‚  â”‚    - feature_1...feature_120                  â”‚          â”‚
â”‚  â”‚    - label (adverse_event_24h)                â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Use Cases:                                   â”‚          â”‚
â”‚  â”‚  âœ“ Model training (large batch reads)        â”‚          â”‚
â”‚  â”‚  âœ“ Batch predictions (daily scoring)         â”‚          â”‚
â”‚  â”‚  âœ“ Feature backfilling                        â”‚          â”‚
â”‚  â”‚  âœ“ Historical analysis                        â”‚          â”‚
â”‚  â”‚  âœ“ DVC versioning                             â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Access Pattern: Scan by date range           â”‚          â”‚
â”‚  â”‚  Retention: 2 years                           â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  ONLINE FEATURE STORE                        â”‚          â”‚
â”‚  â”‚  (Real-time predictions < 200ms)             â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚  Storage: Redis (in-memory key-value)        â”‚          â”‚
â”‚  â”‚  Key Pattern: patient:{patient_id}:features  â”‚          â”‚
â”‚  â”‚  Value: JSON/Hash with latest features       â”‚          â”‚
â”‚  â”‚  TTL: 48 hours (rolling window)              â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Example Entry:                               â”‚          â”‚
â”‚  â”‚  Key: "patient:PT00042:features"             â”‚          â”‚
â”‚  â”‚  Value: {                                     â”‚          â”‚
â”‚  â”‚    "updated_at": "2025-10-07T16:30:00Z",     â”‚          â”‚
â”‚  â”‚    "heart_rate_mean_1h": 78.5,               â”‚          â”‚
â”‚  â”‚    "heart_rate_std_24h": 12.3,               â”‚          â”‚
â”‚  â”‚    "bp_systolic_trend_6h": 2.1,              â”‚          â”‚
â”‚  â”‚    ...120 features...                         â”‚          â”‚
â”‚  â”‚  }                                             â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Use Cases:                                   â”‚          â”‚
â”‚  â”‚  âœ“ Real-time predictions (API serving)       â”‚          â”‚
â”‚  â”‚  âœ“ Fast feature lookup (< 10ms)              â”‚          â”‚
â”‚  â”‚  âœ“ Incremental updates                        â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Access Pattern: Point lookup by patient_id   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â”‚  Sync Strategy:                                             â”‚
â”‚  â€¢ Batch job (hourly): Silver â†’ Offline â†’ Online           â”‚
â”‚  â€¢ Stream processing: Real-time updates to Online           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. MODEL TRAINING (PyTorch)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Training Pipeline reads from OFFLINE FEATURE STORE         â”‚
â”‚  â†“                                                           â”‚
â”‚  Load Parquet files from MinIO (last 90 days)              â”‚
â”‚  â€¢ Train/Val/Test split (70/15/15)                         â”‚
â”‚  â€¢ PyTorch DataLoader                                       â”‚
â”‚  â€¢ Track with MLflow                                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  ML METADATA STORE                            â”‚          â”‚
â”‚  â”‚  (Experiment tracking & model registry)      â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚  Storage: PostgreSQL (mlflow)                â”‚          â”‚
â”‚  â”‚  Tables:                                      â”‚          â”‚
â”‚  â”‚    - experiments                              â”‚          â”‚
â”‚  â”‚    - runs                                     â”‚          â”‚
â”‚  â”‚    - params (learning_rate, epochs, ...)     â”‚          â”‚
â”‚  â”‚    - metrics (train_loss, val_auroc, ...)    â”‚          â”‚
â”‚  â”‚    - tags (git_commit, data_version, ...)    â”‚          â”‚
â”‚  â”‚    - registered_models                        â”‚          â”‚
â”‚  â”‚    - model_versions (staging, production)    â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Artifacts stored in MinIO:                   â”‚          â”‚
â”‚  â”‚    - Model weights (.pth)                     â”‚          â”‚
â”‚  â”‚    - Preprocessing (scaler.pkl)               â”‚          â”‚
â”‚  â”‚    - Confusion matrices                       â”‚          â”‚
â”‚  â”‚    - Feature importance plots                 â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. MODEL SERVING (FastAPI)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API Endpoint: POST /predict                                â”‚
â”‚  Request: {"patient_id": "PT00042"}                        â”‚
â”‚  â†“                                                           â”‚
â”‚  Step 1: Fetch features from ONLINE FEATURE STORE (Redis)  â”‚
â”‚    redis.get("patient:PT00042:features")                   â”‚
â”‚    Latency: < 10ms                                          â”‚
â”‚  â†“                                                           â”‚
â”‚  Step 2: Load model from MLflow Registry                    â”‚
â”‚    model = mlflow.pytorch.load_model("production")         â”‚
â”‚  â†“                                                           â”‚
â”‚  Step 3: Run inference                                      â”‚
â”‚    prediction = model.predict(features)                     â”‚
â”‚  â†“                                                           â”‚
â”‚  Step 4: Log prediction to PostgreSQL                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  PREDICTION LOGS (Audit & monitoring)        â”‚          â”‚
â”‚  â”‚  Storage: PostgreSQL (predictions)           â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚  Table: predictions                           â”‚          â”‚
â”‚  â”‚    - id (UUID)                                â”‚          â”‚
â”‚  â”‚    - patient_id                               â”‚          â”‚
â”‚  â”‚    - timestamp                                â”‚          â”‚
â”‚  â”‚    - prediction_score (0.0-1.0)              â”‚          â”‚
â”‚  â”‚    - risk_level (LOW/MEDIUM/HIGH)            â”‚          â”‚
â”‚  â”‚    - model_version                            â”‚          â”‚
â”‚  â”‚    - latency_ms                               â”‚          â”‚
â”‚  â”‚    - features_used (JSONB)                    â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Indexes:                                     â”‚          â”‚
â”‚  â”‚    - (patient_id, timestamp)                  â”‚          â”‚
â”‚  â”‚    - (timestamp) for drift monitoring         â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  Use Cases:                                   â”‚          â”‚
â”‚  â”‚  âœ“ Audit trail (regulatory compliance)       â”‚          â”‚
â”‚  â”‚  âœ“ Drift detection (compare predictions)     â”‚          â”‚
â”‚  â”‚  âœ“ Performance monitoring                     â”‚          â”‚
â”‚  â”‚  âœ“ Ground truth labeling (when event occurs) â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â”‚  Response: {                                                â”‚
â”‚    "patient_id": "PT00042",                                â”‚
â”‚    "adverse_event_probability": 0.73,                      â”‚
â”‚    "risk_level": "HIGH",                                   â”‚
â”‚    "model_version": "v2.3.1"                               â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. MONITORING & DRIFT DETECTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Monitoring Service (Python)                                â”‚
â”‚  â†“                                                           â”‚
â”‚  Queries PostgreSQL predictions table:                      â”‚
â”‚  â€¢ Calculate rolling 7-day AUROC                           â”‚
â”‚  â€¢ Compare prediction distribution vs training             â”‚
â”‚  â€¢ Detect feature drift (KS test)                          â”‚
â”‚  â€¢ Alert on performance degradation                         â”‚
â”‚  â†“                                                           â”‚
â”‚  Metrics sent to Prometheus â†’ Grafana dashboards           â”‚
â”‚                                                              â”‚
â”‚  If AUROC < 0.80 â†’ Trigger retraining                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. ORCHESTRATION (Airflow)                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  WORKFLOW METADATA STORE                      â”‚          â”‚
â”‚  â”‚  Storage: PostgreSQL (airflow)               â”‚          â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚
â”‚  â”‚  Tables:                                      â”‚          â”‚
â”‚  â”‚    - dag                                      â”‚          â”‚
â”‚  â”‚    - dag_run                                  â”‚          â”‚
â”‚  â”‚    - task_instance                            â”‚          â”‚
â”‚  â”‚    - xcom (inter-task data passing)           â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  DAGs:                                        â”‚          â”‚
â”‚  â”‚  â€¢ data_processing_dag (hourly)               â”‚          â”‚
â”‚  â”‚    - Spark: Bronze â†’ Silver                   â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  â€¢ feature_engineering_dag (hourly)           â”‚          â”‚
â”‚  â”‚    - Read Silver Parquet                      â”‚          â”‚
â”‚  â”‚    - Compute features                         â”‚          â”‚
â”‚  â”‚    - Write to Offline (MinIO)                 â”‚          â”‚
â”‚  â”‚    - Update Online (Redis)                    â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  â€¢ model_monitoring_dag (6-hourly)            â”‚          â”‚
â”‚  â”‚    - Check AUROC                              â”‚          â”‚
â”‚  â”‚    - Detect drift                             â”‚          â”‚
â”‚  â”‚    - Alert if needed                          â”‚          â”‚
â”‚  â”‚                                                â”‚          â”‚
â”‚  â”‚  â€¢ model_retraining_dag (triggered)           â”‚          â”‚
â”‚  â”‚    - Load from Offline Feature Store          â”‚          â”‚
â”‚  â”‚    - Train new model                          â”‚          â”‚
â”‚  â”‚    - Evaluate & promote                       â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ“Š What It Does
The script checks all 6 levels of your pipeline:
Level 0: Infrastructure

âœ… MinIO server accessible
âœ… MinIO bucket exists
âœ… Kafka broker accessible
âœ… Redis server accessible
âœ… MLflow server accessible
âœ… PostgreSQL databases accessible

Level 1: Data Ingestion

âœ… Kafka producer running
âœ… Kafka consumer running
âœ… Kafka topics exist
âœ… Bronze data exists

Level 2: Data Processing

âœ… Spark master/worker running
âœ… Spark accessible
âœ… Silver data exists

Level 3: Feature Engineering

âœ… Feature engineering service running
âœ… Offline features exist (MinIO)
âœ… Online features exist (Redis)

Level 4: ML Pipeline

âœ… Model serving API running
âœ… Health endpoint accessible

Level 5: Observability

âœ… Prometheus running
âœ… Grafana running
âœ… Services accessible

Data Flow Validation

ğŸ“Š Counts files in Bronze layer
ğŸ“Š Counts files in Silver layer
ğŸ“Š Counts files in Feature store

```sh
# Use MinIO client to browse entire bucket
docker exec -it minio sh -c "mc alias set local http://localhost:9000 minioadmin minioadmin && mc ls --recursive local/clinical-mlops/"
```