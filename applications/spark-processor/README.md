## ğŸ§  `spark/` â€” Spark Services for Clinical MLOps

This folder contains the Docker setup for **Apache Spark 3.5.4 (Java 17)** used in the Clinical MLOps data pipeline.

---

### âš™ï¸ Components

| Service             | Purpose                                                                                |
| ------------------- | -------------------------------------------------------------------------------------- |
| **spark-master**    | Spark master node â€” web UI at [http://localhost:8080](http://localhost:8080)           |
| **spark-worker**    | Worker node executing distributed tasks                                                |
| **spark-streaming** | Real-time processor consuming Kafka topics and writing to the Bronze â†’ Silver pipeline |
| **spark-batch**     | Batch processor converting Bronze â†’ Silver data hourly (continuous loop)               |

---

### ğŸš€ Build & Run

```bash
# Build all Spark images
docker compose build spark-master spark-worker spark-streaming spark-batch

# Start Spark cluster
docker compose up -d spark-master spark-worker spark-streaming spark-batch
```

Check container status:

```bash
docker ps
```

View logs (continuous stream):

```bash
docker logs -f spark-batch
```

---

### ğŸ” Continuous Batch Processor

The `spark-batch` service now runs continuously using
[`entrypoint.sh`](./entrypoint.sh).
It executes the batch job every `INTERVAL_MINUTES` (default = 60 min).

Change interval:

```bash
# Example: run every 30 minutes
docker compose up -d spark-batch
# or override at runtime
docker run -e INTERVAL_MINUTES=30 clinical-mlops/spark-batch
```

Youâ€™ll see log messages like:

```
â° Starting batch job at Thu Oct 09 17:00:00
âœ… Batch completed at Thu Oct 09 17:01:10
Sleeping 60 minutes before next run...
```

---

### ğŸ§¹ Maintenance Commands

```bash
# Stop Spark containers
docker compose stop spark-master spark-worker spark-streaming spark-batch

# Remove Spark containers + volumes
docker compose down -v

# View Spark UI
open http://localhost:8080   # Master UI
open http://localhost:4040   # Driver UI (active batch job)
```

---

### ğŸ§© Project Structure

```
spark/
â”œâ”€â”€ Dockerfile                # Base image for Spark Batch
â”œâ”€â”€ entrypoint.sh             # Looping scheduler (runs spark-submit every hour)
â”œâ”€â”€ jobs/                     # Python Spark jobs (e.g., bronze_to_silver_batch.py)
â”œâ”€â”€ transformations/          # Data transformation logic
â”œâ”€â”€ utils/                    # Helper functions and connectors
â””â”€â”€ README.md                 # â† you are here
```

---

### âœ… Notes

* All jobs use the MinIO endpoint `http://minio:9000` via the `s3a://` connector.
* Update credentials or paths in `entrypoint.sh` if MinIO changes.
* Future integration: Airflow or Dagster can trigger `spark-batch` on a schedule instead of the internal loop.

---
