networks:
  mlops-network:
    driver: bridge

volumes:
  minio-data:
  postgres-mlflow-data:
  postgres-airflow-data:
  kafka-data:
  zookeeper-data:
  prometheus-data:
  grafana-data:
  redis-data:
  spark-logs:
  elasticsearch-data:
  logstash-data:
  redis-insight-data:
  feature-data:

services:
  # ============================================================================
  # LEVEL 0: INFRASTRUCTURE (NO PROFILES - Always Available)
  # ============================================================================

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z-cpuv1
    container_name: minio
    ports:
      - "9000:9000"     # API endpoint
      - "9001:9001"     # Web Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio-data:/data
    entrypoint: >
      /bin/sh -c "
      echo 'üöÄ Starting MinIO server...';
      apk add --no-cache curl;
      minio server /data --console-address ':9001' --address ':9000' &
      MINIO_PID=$$!;
      
      echo '‚è≥ Waiting for MinIO to be ready...';
      until mc ready local 2>/dev/null; do sleep 1; done;
      
      echo 'üîß Configuring default alias...';
      mc alias set myminio http://localhost:9000 minioadmin minioadmin;
      
      echo '‚úÖ MinIO ready with pre-configured alias';
      wait $$MINIO_PID;
      "
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "mc", "ready", "local"] 
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 20s
    restart: unless-stopped

  minio-setup:
    image: minio/mc:RELEASE.2025-08-13T08-35-41Z-cpuv1
    container_name: minio-setup
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - mlops-network
    entrypoint: /bin/sh
    command:
      - -c
      - |
        echo '‚è≥ Waiting for MinIO to become ready...'
        until mc alias set myminio http://minio:9000 minioadmin minioadmin 2>/dev/null; do
          echo 'Waiting for MinIO...'
          sleep 2
        done
        echo 'üîß Configuring MinIO client (mc)...'
        mc alias set myminio http://minio:9000 minioadmin minioadmin
        echo 'üì¶ Creating buckets...'
        mc mb myminio/clinical-mlops --ignore-existing
        mc mb myminio/mlflow-artifacts --ignore-existing
        mc mb myminio/dvc-storage --ignore-existing
        echo 'üåç Applying anonymous download policies...'
        mc anonymous set download myminio/clinical-mlops
        mc anonymous set download myminio/mlflow-artifacts
        mc anonymous set download myminio/dvc-storage
        echo '‚úÖ Buckets successfully configured:'
        mc ls myminio/
    restart: "no"


  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "8090:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
    networks:
      - mlops-network

  postgres-mlflow:
    image: postgres:15-alpine
    container_name: postgres-mlflow
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
      POSTGRES_DB: mlflow
    volumes:
      - postgres-mlflow-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-airflow:
    image: postgres:15-alpine
    container_name: postgres-airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis-insight:
    image: redis/redisinsight:latest
    container_name: redis-insight
    ports:
      - "5540:5540"
    volumes:
      - redis-insight-data:/data
    networks:
      - mlops-network
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:5540/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3


  # ============================================================================
  # LEVEL 1: DATA INGESTION (Profile: data-ingestion)
  # ============================================================================
  
  kafka-producer:
    build:
      context: ./applications/kafka-producer
      dockerfile: Dockerfile
    container_name: kafka-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      PRODUCER_RATE: 100  # messages per second
      NUM_PATIENTS: 1000
    networks:
      - mlops-network
    restart: unless-stopped
    profiles:
      - data-ingestion

  kafka-consumer:
    build:
      context: ./applications/kafka-consumer
      dockerfile: Dockerfile
    container_name: kafka-consumer
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_GROUP_ID: clinical-consumer-group
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: minioadmin
      S3_SECRET_KEY: minioadmin
      S3_BUCKET: clinical-mlops
      S3_PREFIX: raw/
    networks:
      - mlops-network
    restart: unless-stopped
    profiles:
      - data-ingestion

  clinical-mq:
    platform: linux/amd64
    build:
      context: ./infrastructure/docker/ibm-mq
      dockerfile: Dockerfile
    container_name: clinical-mq
    environment:
      - LICENSE=accept
      - MQ_QMGR_NAME=CLINICAL_QM
      - MQ_APP_PASSWORD=${MQ_APP_PASSWORD:-clinical123}
      - MQ_ADMIN_PASSWORD=${MQ_ADMIN_PASSWORD:-admin123}
      - MQ_ENABLE_EMBEDDED_WEB_SERVER=true
    ports:
      - "1414:1414"
      - "9443:9443"
    healthcheck:
      test: ["CMD-SHELL", "dspmq -m CLINICAL_QM -o status | grep -q 'STATUS(Running)'"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    restart: unless-stopped
    networks:
      - mlops-network
    profiles:
      - data-ingestion

  clinical-data-gateway:
    build:
      context: ./applications/clinical-data-gateway
      dockerfile: Dockerfile
    container_name: clinical-gateway
    environment:
      - SPRING_PROFILES_ACTIVE=${SPRING_PROFILES_ACTIVE:-development}
      - SERVER_PORT=${SERVER_PORT:-8080}
      - IBM_MQ_QUEUE_MANAGER=${IBM_MQ_QUEUE_MANAGER:-CLINICAL_QM}
      - IBM_MQ_CHANNEL=${IBM_MQ_CHANNEL:-DEV.APP.SVRCONN}
      - IBM_MQ_CONNNAME=${IBM_MQ_CONNNAME:-clinical-mq(1414)}
      - IBM_MQ_USER=${IBM_MQ_USER:-app}
      - IBM_MQ_PASSWORD=${IBM_MQ_PASSWORD:-clinical123}
      - IBM_MQ_USERAUTHENTICATIONMQCSP=${IBM_MQ_USERAUTHENTICATIONMQCSP:-true}
    depends_on:
      clinical-mq:
        condition: service_healthy
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:${SERVER_PORT:-8080}/actuator/health"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 60s
    restart: unless-stopped
    networks:
      - mlops-network
    profiles:
      - data-ingestion

  lab-results-processor:
    build:
      context: ./applications/lab-results-processor
      dockerfile: Dockerfile
    container_name: lab-processor
    environment:
      - SPRING_PROFILES_ACTIVE=${SPRING_PROFILES_ACTIVE:-development}
      - SERVER_PORT=8081
      - IBM_MQ_QUEUE_MANAGER=${IBM_MQ_QUEUE_MANAGER:-CLINICAL_QM}
      - IBM_MQ_CHANNEL=${IBM_MQ_CHANNEL:-DEV.APP.SVRCONN}
      - IBM_MQ_CONNNAME=clinical-mq(1414)
      - IBM_MQ_USER=${IBM_MQ_USER:-app}
      - IBM_MQ_PASSWORD=${IBM_MQ_PASSWORD:-clinical123}
      - IBM_MQ_USERAUTHENTICATIONMQCSP=${IBM_MQ_USERAUTHENTICATIONMQCSP:-true}
    depends_on:
      clinical-mq:
        condition: service_healthy
    ports:
      - "8083:8081"
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8081/actuator/health"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 60s
    restart: unless-stopped
    networks:
      - mlops-network
    profiles:
      - data-ingestion

  clinical-data-generator:
    build:
      context: .
      dockerfile: operations/docker/clinical-data-generator/Dockerfile
    container_name: clinical-data-generator
    environment:
      - GEN_ENDPOINT=http://clinical-gateway:8080
      - GEN_MODE=${GEN_MODE:-random}
      - GEN_INTERVAL=${GEN_INTERVAL:-1-5}
      - GEN_COUNT=${GEN_COUNT:-0}
      - GEN_DATA_TYPE=${GEN_DATA_TYPE:-random}
      - GEN_START_DELAY=${GEN_START_DELAY:-30}
    restart: on-failure
    networks:
      - mlops-network
    profiles:
      - data-ingestion

  # ============================================================================
  # LEVEL 2: DATA PROCESSING (Profile: data-processing)
  # ============================================================================

  spark-master:
    image: apache/spark:3.5.4-java17
    container_name: spark-master
    user: root
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8080:8080"
      - "7077:7077"
      - "6066:6066"
    networks:
      - mlops-network
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - data-processing

  spark-worker:
    image: apache/spark:3.5.4-java17
    container_name: spark-worker
    user: root
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8081:8081"
    networks:
      - mlops-network
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - data-processing

  spark-streaming:
    build:
      context: ./applications/spark-processor
      dockerfile: Dockerfile.streaming
    container_name: spark-streaming
    depends_on:
      spark-master:
        condition: service_healthy
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - S3_ENDPOINT=http://minio:9000
      - S3_ACCESS_KEY=minioadmin
      - S3_SECRET_KEY=minioadmin
      - S3_BUCKET=clinical-mlops
    networks:
      - mlops-network
    restart: unless-stopped
    profiles:
      - data-processing

  spark-batch:
    build:
      context: ./applications/spark-processor
      dockerfile: Dockerfile.batch
    container_name: spark-batch
    depends_on:
      spark-master:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      - S3_ENDPOINT=http://minio:9000
      - S3_ACCESS_KEY=minioadmin
      - S3_SECRET_KEY=minioadmin
      - S3_BUCKET=clinical-mlops
      - PROCESS_HOURS=1
    networks:
      - mlops-network
    profiles:
      - batch-job          # Keep backward compatibility
      - data-processing    # Add new level-based profile

  # ============================================================================
  # LEVEL 3: FEATURE ENGINEERING (Profile: features)
  # ============================================================================

  feature-engineering:
    build:
      context: ./applications/feature-engineering
      dockerfile: Dockerfile
    container_name: feature-engineering
    depends_on:
      - minio
      - redis
    environment:
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: minioadmin
      S3_SECRET_KEY: minioadmin
      S3_BUCKET: clinical-mlops
      REDIS_HOST: redis
      REDIS_PORT: 6379
      PROCESS_DATE: ${PROCESS_DATE:-}
    networks:
      - mlops-network
    volumes:
      - feature-data:/app/data 
    profiles:
      - feature-engineering  # Keep backward compatibility
      - features             # Add new level-based profile


  # ============================================================================
  # LEVEL 4: ML PIPELINE (Profile: ml-pipeline)
  # ============================================================================

  mlflow-server:
    image: python:3.11-slim
    container_name: mlflow-server
    depends_on:
      postgres-mlflow:
        condition: service_healthy
      minio:
        condition: service_healthy
    ports:
      - "5050:5000"
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      GUNICORN_CMD_ARGS: "--bind 0.0.0.0:5000 --workers 4 --timeout 120"
    networks:
      - mlops-network
    command: >
      sh -c '
      pip install -q --no-cache-dir mlflow==2.8.1 boto3 psycopg2-binary &&
      mlflow server
      --backend-store-uri postgresql://mlflow:mlflow@postgres-mlflow:5432/mlflow
      --default-artifact-root s3://mlflow-artifacts/
      --host 0.0.0.0
      --port 5000
      '

    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - ml-pipeline    # THIS IS THE KEY CHANGE - adds the profile

  ml-training:
    build:
      context: ./applications/ml-training
      dockerfile: Dockerfile
    container_name: ml-training
    depends_on:
      mlflow-server:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      MLFLOW_TRACKING_URI: http://mlflow-server:5000
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: minioadmin
      S3_SECRET_KEY: minioadmin
      S3_BUCKET: clinical-mlops
      REDIS_HOST: redis
      REDIS_PORT: 6379
    networks:
      - mlops-network
    volumes:
      - ./applications/ml-training:/app
    profiles:
      - training       # Keep backward compatibility
      - ml-pipeline    # Add new level-based profile

  model-serving:
    build:
      context: ./applications/model-serving
      dockerfile: Dockerfile
    container_name: model-serving
    depends_on:
      mlflow-server:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8000:8000"
    environment:
      MLFLOW_TRACKING_URI: http://mlflow-server:5000
      REDIS_HOST: redis
      REDIS_PORT: 6379
      MODEL_NAME: adverse-event-predictor
      MODEL_STAGE: production
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - serving        # Keep backward compatibility
      - ml-pipeline    # Add new level-based profile

  # ============================================================================
  # LEVEL 5: ORCHESTRATION (Profile: orchestration)
  # ============================================================================

  airflow-init:
    build:
      context: ./orchestration/airflow
      dockerfile: Dockerfile
    container_name: airflow-init
    depends_on:
      postgres-airflow:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'UKMzEm3yIuFYEq1y3-2FxGb6l6_kU5KDo5YN6RnGq3Q='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
    networks:
      - mlops-network
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true
    profiles:
      - orchestration

  airflow-webserver:
    build:
      context: ./orchestration/airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'UKMzEm3yIuFYEq1y3-2FxGb6l6_kU5KDo5YN6RnGq3Q='
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    ports:
      - "8085:8080"
    networks:
      - mlops-network
    # volumes:
    #   - ./orchestration/airflow/dags:/opt/airflow/dags
    #   - ./orchestration/airflow/plugins:/opt/airflow/plugins
    #   - ./orchestration/airflow/config:/opt/airflow/config
    # Note: Volumes commented out for dev container compatibility
    # Files are baked into image during build (see Dockerfile)
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - orchestration

  airflow-scheduler:
    build:
      context: ./orchestration/airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'UKMzEm3yIuFYEq1y3-2FxGb6l6_kU5KDo5YN6RnGq3Q='
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    networks:
      - mlops-network
    # volumes:
    #   - ./orchestration/airflow/dags:/opt/airflow/dags
    #   - ./orchestration/airflow/plugins:/opt/airflow/plugins
    #   - ./orchestration/airflow/config:/opt/airflow/config
    # Note: Volumes commented out for dev container compatibility
    # Files are baked into image during build (see Dockerfile)
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $$(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - orchestration

  prometheus:
    image: prom/prometheus:v3.6.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/alerts:/etc/prometheus/alerts
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - observability

  grafana:
    image: grafana/grafana:12.2-ubuntu
    container_name: grafana
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: ''
    volumes:
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - grafana-data:/var/lib/grafana
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - observability

  monitoring-service:
    build:
      context: ./applications/monitoring-service
      dockerfile: Dockerfile
    container_name: monitoring-service
    depends_on:
      - prometheus
      - model-serving
    environment:
      PROMETHEUS_URL: http://prometheus:9090
      MODEL_SERVING_URL: http://model-serving:8000
      MLFLOW_TRACKING_URI: http://mlflow-server:5000
      ALERT_WEBHOOK_URL: http://airflow-webserver:8080/api/v1/dags/model_monitoring/dagRuns
      CHECK_INTERVAL_SECONDS: 3600
    networks:
      - mlops-network
    profiles:
      - monitoring       # Keep backward compatibility
      - observability    # Add new level-based profile

  opensearch:
    image: opensearchproject/opensearch:2.14.0
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - plugins.security.disabled=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
      - "9600:9600"
    volumes:
      - elasticsearch-data:/usr/share/opensearch/data
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - observability

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.14.0
    container_name: opensearch-dashboards
    depends_on:
      opensearch:
        condition: service_healthy
    environment:
      - OPENSEARCH_HOSTS=http://opensearch:9200
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    ports:
      - "5601:5601"
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - observability

  data-prepper:
    image: opensearchproject/data-prepper:2.8.0
    container_name: data-prepper
    depends_on:
      opensearch:
        condition: service_healthy
    ports:
      - "2021:2021"
      - "21890:21890"
    volumes:
      - ./monitoring/opensearch/data-prepper/pipelines:/usr/share/data-prepper/pipelines
      - ./monitoring/opensearch/data-prepper/config:/usr/share/data-prepper/config
    networks:
      - mlops-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:2021 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles:
      - observability

  filebeat:
    image: elastic/filebeat:8.18.8
    container_name: filebeat
    user: root
    depends_on:
      data-prepper:
        condition: service_started
    volumes:
      - ./monitoring/elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: filebeat -e -strict.perms=false
    networks:
      - mlops-network
    profiles:
      - observability
